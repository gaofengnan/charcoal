library(glmnet)
library(MASS)
library(RSpectra)
library(evd)

#' Soft thresholding a vector
#' @param x a vector of real numbers
#' @param lambda soft thresholding value
#' @return a vector of the same length
#' @description entries of v are moved towards 0 by the amount lambda until 
#' they hit 0. From github.com/wangtengyao/putils.
#' @export
vector.soft.thresh <- function(x, lambda){
  sign(x)*pmax(0,(abs(x)-lambda))
}

#' Hard thresholding a vector
#' @param x a vector of real numbers
#' @param lambda hard thresholding value
#' @return a vector of the same length
#' @description entries of v that are below lambda are set to 0. 
#' From github.com/wangtengyao/putils.
#' @export
vector.hard.thresh <- function(x, lambda){
  x[abs(x)<lambda] <- 0; x
}


#' Computing the orthogonal matrix spanning the orthogonal complement of the 
#' range of X
#' @param X an n x p matrix with n > p
#' @return matrix A of size n x (n-p) with orthogonal columns
#' @description Generate the orthogonal complement of the range of matrix X
#' @export
orthogonalProjection <- function(X){
  n <- dim(X)[1]; p <- dim(X)[2]
  qr.Q(qr(X), complete=TRUE)[, (p+1):n]
}




#' Generate a sequential linear regression sample with a single changepoint or 
#' multiple changepoints
#' @param n the total number of data points
#' @param tau a number (in case of single changepoint) or vector containing all changepoints in fraction. If tau = c(0.1,0.2), the changes takes place at 0.1n 
#' and 0.2n
#' @param k a vector containing the sparsity of each vector of change, if k is 
#' of length 1, then the same sparsity is applied to all change vectors. E.g., 
#' k = c(3,5), the first change vector is 3-sparse and the second 5-sparse
#' @param rho signal strength at each changepoint, i.e. the l2 norm of each 
#' change vector, if rho is of length 1, then the signal strength is applied to 
#' all changes
#' @param design a string specifying the distribution of the random design, 
#' implemented options include "gaussian" for GOE design;"t4" for t distribution
#'  with degree of freedom 4 (4 can be replaced by any desired df, and if 
#'  unspecified, df of 4 is applied); "correlated" or "ar" for each row 
#'  generated by N_p(0,Sigma), where Sigma has (i,j) entry of 0.7^(i-j); 
#'  "anova" for one-way anova design; and "rademacher" for all entries of X 
#'  being independent fair coin toss between {-1,1}
#' @param error a string specifying the distribution of observational errors 
#' in linear regression, the implemented options include "gaussian" for 
#' standard normal errors;"t4" for t distribution with degree of freedom 4 
#' (4 can be replaced by any desired df, and if unspecified, df of 4 is 
#' applied); "exp" the centered exponential distribution; "laplace" for 
#' standardized Laplace distribution with zero mean and std 1; and "rademacher" 
#' for all errors of being independent fair coin toss between {-1,1}
#' @param seed the random seed for the ensuing random interval generation for 
#' the narrowest-over-threshold. The default is NULL, where no seed is set.
#' @param sigma the regression noise level, uniform among all time points
#' @return a list containing (X, Y, beta, tau). X as the design matrix and Y as 
#' the response vector, beta as the regression coefficient matrix with each 
#' column corresponding to each time point, tau as the fraction of n where 
#' true changes take place
#' @description Generate a sequential linear regression sample with multiple 
#' changepoints, with options for different distributions of both random design
#' and observational error
#' @export
simulateMultiCP <- function(n, tau, p, k, rho,
                            design='gaussian',
                            error='gaussian', sigma=1, seed=NULL){

  tmp <- stringr::str_match(error, '^([a-z]*?)([0-9\\.]*?)$')
  error <- tmp[1,2]
  df <- as.numeric(tmp[1,3])
  if (is.na(df)) df <- 4 # if unspecified, t has df 4
  tmp <- stringr::str_match(design, '^([a-z]*?)([0-9\\.]*?)$')
  design <- tmp[1,2]
  design.df <- as.numeric(tmp[1,3])
  if (is.na(design.df)) design.df <- 4
  no_change <- length(tau)
  cps <- c(floor(tau *n),n)

  if(length(rho)!=1 && length(rho)!=no_change) {
    print("the length of rho does NOT match that of tau")
    return(NULL)
  } else if (length(k)!=1 && length(k)!=no_change) {
    print("the length of k does NOT match that of tau")
  }

  if (!is.null(seed)) set.seed(seed)

  if (design == 'correlated' || design =='ar') {
    Sigma <- outer(1:p, 1:p, function(i,j){0.8^abs(i-j)})
    X <- MASS::mvrnorm(n, rep(0,p), Sigma)
  } else if (design == 'rademacher') {
    X <- matrix((2*rbinom(n*p,1,1/2) - 1), nrow = n)
  } else if (design == 'anova') {
    X <- matrix(0, n, p)
    X[cbind(1:(n), 1:p)] <- 1
  } else if ('t'==design) {
    X <- matrix(rt(n*p,df=design.df)/sqrt(design.df/(design.df-2)),nrow=n)
  } else { X <- matrix(rnorm(n*p), nrow=n) }

  if (error == 't'){
    e <- rt(n, df=df) / sqrt(df/(df-2))
  } else if (error == 'rademacher') {
    e <- (2*rbinom(n,1,1/2) - 1)
  } else if (error == 'laplace'){
    e <- rexp(n) * (2*rbinom(n,1,1/2) - 1) / sqrt(2)
  } else if (error == 'exp'){
    e <- (rexp(n) - 1)
  } else { e <- rnorm(n) }

  theta_mat <- rbind(k, matrix(-Inf, nrow=p, ncol=no_change))
  theta_mat <- apply(theta_mat, 2, {function (v) c(v[1], rnorm(v[1]),
                                                   rep(0,length(v)-1 -v[1]))})
  theta_mat <- as.matrix(theta_mat[-1,])
  theta_mat <- sweep(theta_mat, 2 , sqrt(colSums(theta_mat^2))/rho,
                     FUN='/') #column normalize so each column has proper signal
  if (no_change!=1) theta_mat <- apply(theta_mat, 2, sample)
  theta_mat_cumsum <- t(apply(cbind(0, theta_mat), 1, cumsum))
  beta_idx <- rbind(1:n, matrix(-Inf,nrow=1,ncol=n))
  beta_idx <- apply(beta_idx, 2, {function (v) c(v[1], 1 + sum(v[1] > cps))})
  beta <- theta_mat_cumsum[, beta_idx[2,]] + rnorm(p)*max(rho,1)


  Y <- apply(X * t(beta), 1, sum) + e * sigma # faster
  # Y <- diag(X %*% beta) + e

  return(list(X = X, Y = Y, beta = beta, tau = tau))
}





#' Computing the noise variance in a high-dimensional linear model
#' @param W design matrix for high-dimensional linear model
#' @param z response in a high-dimensional linear model
#' @return a nonnegative scalar of estimated noise standard deviation
#' @description Assume data are generated from the model z = W b + e, where e 
#' has independent components N(0, s^2), we estimate s^2 by fitting a 
#' cross-validated Lasso estimator of b and then estimate sigma from the 
#' residual sum of squares.
#' @export
noise_sd <- function(W, z){
  fit.cv <- glmnet::cv.glmnet(W,z,intercept=F,standardize=F)
  betahat <- coef(fit.cv, s='lambda.min')
  Xbetahat <- predict(fit.cv, newx=W, s='lambda.min')
  sqrt(sum((z - Xbetahat)^2) / (nrow(W) - sum(betahat!=0)))
}

#' Computing the noise variance in a high-dimensional linear model
#' @param W design matrix for high-dimensional linear model
#' @param z response in a high-dimensional linear model
#' @return a nonnegative scalar of estimated noise standard deviation
#' @description Assume data are generated from the model z = W b + e, where e 
#' has independent components N(0, s^2), we estimate s^2 by using the method 
#' in Dicker (2014).  If the method fails, a fallback with nosie_sd will be 
#' used.
#' @references Dicker, L. H. (2014). Variance estimation in high-dimensional 
#' linear models. Biometrika, 101(2), 269-284.
#' @export
dicker_noise_sd <- function(W, z)
{
  m <- nrow(W); p <- ncol(W)
  gram.W.norm <- t(W) %*% W/m
  m1.hat <- sum(diag(gram.W.norm))/p
  m2.hat <- sum(gram.W.norm^2)/p - p * m1.hat^2 / m
  sigma.tilde.square <- (1 + p*m1.hat^2/(m+1)/m2.hat)*sum(z^2)/m - 
    m1.hat*sum((t(W) %*% z)^2)/m/(m+1)/m2.hat
  if (sigma.tilde.square > 0){
    return(sqrt(sigma.tilde.square))
  } else return(noise_sd(W,z))
}


#' Internal function to generate random intervals for not_cpreg, the main 
#' multiple changepoint estimation function in linear regressions as in 
#' Algorithm 4
#' @param n the sample size in question
#' @param p the regression coefficient dimension in question
#' @param no_intervals the desired number of intervals
#' @param delta only intervals with length longer than (1+delta) * p will be 
#' kept
getNOTIntervals <- function(n, p, no_intervals, delta=0.1){
  intervals <- matrix(sample.int(n, no_intervals*100, replace=TRUE), ncol = 2)
  intervals <- t(apply(intervals, 1, sort))
  intervals <- intervals[intervals[,2] - intervals[,1] >= (1 + delta)*p, ]
  if (nrow(intervals) > no_intervals) intervals <- intervals[1:no_intervals, ]
}


#' Generate the threshold for Algorithm 4 (and possibly testing) in 
#' Gao and Wang (2022)
#' @param n the sample size, i.e., the number of rows in the design
#' @param p the regression coefficient dimension
#' @param burnIn the burnIn parameter to be passed the cpreg function
#' @param alpha the critical type-I error probability threshold, should in 
#' practice adjusted for the multiple testing, i.e., in this case the number 
#' of intervals
#' @param sigma the standard deviation of the noise in the regressions
#' @param b the standard deviation of each entry of the randomly generated 
#' regression coefficients, which follows N(0,b^2)
#' @param verbose whether to output intermediate results/progress bar in the 
#' console
#' @return a nonnegative scalar as the threshold for not_cpreg
#' @description Generate the threshold for combining charcoal with the 
#' narrowest-over-threshold algorithm, which is also used as the critical 
#' threshold in the testing refining stage in not_cpreg.  It works by generating
#'  permSize Monte Carlo repetitions of running cpreg on the null model without 
#'  any change in regression coefficients
#' @export
getTestThreshold <- function(n, p, burnIn=0, alpha=0.05, permSize=1000, sigma=1,
                             b=1, verbose=FALSE) {
  # sigma --- regression noise level
  # b --- baseline regression coefficient magnitude level
  
  # n <- nrow(X); p <-ncol(X)
  cpreg_stats <- rep(-Inf, permSize)
  for (i in 1:permSize) {
    X <- matrix(rnorm(n*p), nrow=n)
    beta <- rnorm(p) * max(b,1)
    Y <- X %*% beta + rnorm(n) * sigma
    tmp <- cpreg(X, Y, sigma=sigma, burnIn=burnIn)
    cpreg_stats[i] <- tmp$stats[tmp$cp]
    if (verbose) printPercentage(i, permSize)
  }
  
  if (alpha * permSize >= 1){
    zeta <- quantile(cpreg_stats, 1 - alpha)
  } else {
    gev.fit <- evd::fgev(cpreg_stats)$param
    zeta <- evd::qgev(1 - alpha, loc=gev.fit[1], scale=gev.fit[2],
                      shape=gev.fit[3])
  }
  return(list(thresh=zeta, stats=cpreg_stats))
}


#' Main function implementing the charcoal algorithms for single changepoint
#' @param X design matrix of the linear regression model
#' @param Y response vector of the linear regression model
#' @param lambda threshold for Algorithms 1 and 2, set it to NULL and the 
#' recommended value is applied
#' @param sigma noise standard deviation of the regression models; if unknown, 
#' set to NULL.
#' @param aggregate_method specifies which variant of the algorithms to be 
#' applied. Set it to 'compsket' for Algorithm 1, to 'proj' for Algorithm 2, 
#' and to 'lasso_bic' for Algorithm 3.
#' @param burnIn specifies the fraction at both ends of the interval to be 
#' discarded as possible changes, to handle common boundary effects
#' @param verbose whether to output intermidate results/progress bar in the 
#' console
#' @return a list containing the test statistics and changepoint estimate.
#' @description From the model y_t = x_t beta_1 + eps_t for 1<=t<=z and 
#' y_t = x_t beta_2 + eps_t for z+1<=t<=n, where x_t are p-dimensional design 
#' vectors for 1<=t<=n, we estimate the changepoint z where the change of 
#' regression coefficient takes place. The localization is performed using the 
#' charcoal algorithms from Gao and Wang (2022). The implementation includes
#' Algorithms 1, 2 and 3.
#' @references Gao, F. and Wang, T. (2022) Sparse change detection in 
#' high-dimensional linear regression.
#' @export
#' @examples
#' # problem parameters
#' n <- 240 # total sample size
#' z <- 150  # changepoint location
#' p <- 160 # dimension of covariates
#' k <- 10 # sparsity of difference of the two regression coefficients
#' rho <- 1 # difference in l_2 norm of the two regression coefficients
#' # generate design matrices
#' X1 <- matrix(rnorm(z * p), z, p)
#' X2 <- matrix(rnorm((n-z) * p), (n-z), p)
#' # generate regression coefficients
#' beta1 <- rnorm(p)
#' theta <- c(rnorm(k), rep(0, p-k))
#' theta <- theta / sqrt(sum(theta^2)) * rho
#' beta2 <- beta1 + theta
#' # generate response vectors
#' y1 <- X1 %*% beta1 + rnorm(z)
#' y2 <- X2 %*% beta2 + rnorm(n-z)
#' # combine the samples before and after changepoint to form the
#' # response and covariates
#' X <- rbind(X1,X2)
#' Y <- rbind(y1,y2)
#' # look for changepoints
#' result <- cpreg(X,Y)
#' # or  result <- cpreg(X,Y,burnIn=0.05,aggregate_method='lasso_bic')
#' # Alternatively, X and Y can be generated by
#' ret <- simulateMultiCP(n,z/n,p,k,rho,sigma=1); X <- ret$X; Y <- ret$Y
cpreg <- function(X, Y, lambda=NA, sigma=NULL, burnIn=0,
                  aggregate_method='proj', verbose=FALSE){
  
  n <- dim(X)[1]; p <- dim(X)[2]; m <- n - p
  cross_validation <- TRUE
  if (p>=n) return(list(stats=NULL,cp=NULL))
  if (!is.null(sigma)) {X <- X/sigma; Y<- Y/sigma}
  
  if (is.na(lambda)) lambda <- 0.5*log(p) # default setting
  
  A <- t(orthogonalProjection(X))
  Z <- A %*% Y
  
  W <- matrix(0, m, p)
  Q <- matrix(0, p, n-1)
  stats <- rep(-Inf, n-1)
  coeffs <- matrix(0, p, n-1)
  
  if (grepl('lasso',aggregate_method)) {
    for (t in 1:(n-1)) {
      W <-  W + 2 * outer(A[,t], X[t,])
      
      if (t >= 5 && t <= n-5){
        if (cross_validation) {
          tmp <- glmnet::cv.glmnet(W, Z,nfolds=5, intercept=FALSE,
                                   standardize=TRUE, parallel = FALSE)
          b <- as.vector(coef(tmp, s=tmp$lambda.min))[-1]
        } else {
          tmp <- glmnet::glmnet(W,Z, intercept=FALSE,
                                lambda = sqrt(max(log(p/(0.25*m)),1)))
          b <- as.vector(coef(tmp))[-1]
        }
        coeffs[, t] <- b
        if (grepl('bic',aggregate_method)) {
          stats[t] <- -(sum((Z - W %*% b)^2) + log(m) * sum(b!=0))
        } else stats[t] <- -(sum((Z - W %*% b)^2))
      }
      if (verbose) printPercentage(t, n-1)
    } } else {
      col_scales <- matrix(0, p, n-1)
      for (t in 1:(n-1)) {
        W <- W + outer(A[,t], X[t,])
        col_scales[,t] <- sqrt(colSums(W^2))
      }
      Q <- X[-n, ] * colSums(A[, -n] * as.vector(Z))
      Q <- t(apply(Q, 2, cumsum)) / col_scales
    }
  
  idx_begin <- (round(burnIn*n)+1)
  idx_end <- (-round(burnIn*n)+n-1)
  
  lambda <- lambda * mad(Q)
  if (aggregate_method=='compsket'){
    stats <- sqrt(colSums(vector.soft.thresh(Q[, ], lambda)^2))
  } else if (aggregate_method=='svd'){
    Q.thresh <- vector.soft.thresh(Q[, ], lambda)
    v <- svd(Q.thresh)$v[,1]
    stats <- abs(v)
  } else if (aggregate_method=='proj'){
    Q.thresh <- vector.soft.thresh(Q[, ], lambda)
    # u <- svd(Q.thresh)$u[,1]
    u <- RSpectra::svds(Q.thresh, 1)$u
    stats <- as.vector(abs(t(u)%*%Q[, ]))
  } else if (grepl('lasso', aggregate_method)) {
    attr(stats, 'coeff') <- coeffs[,]
  } else {
    stats <- NA
  }
  
  cp <- which.max(stats[(idx_begin:idx_end)]) +idx_begin - 1
  return(list(cp=cp, stats=stats, Q=Q, besidesBurned=c(idx_begin,idx_end)))
}

 
#' Main function implementing the charcoal algorithm for multiple changepoints

#' @param X design matrix of the linear regression model
#' @param Y response vector of the linear regression model
#' @param sigma noise standard deviation of the regression models (must be 
#' supplied)
#' @param no_intervals number of intervals for the narrowest-over-threshold part
#' @param seed the random seed for the ensuing random interval generation for 
#' the narrowest-over-threshold. The default is NULL, where no seed is set.
#' @param Thresh the threshold for the narrowest-over-threshold, and the 
#' critical value in the testing refinement stage
#' @param cpreg_method specifies which variant of the algorithms to be applied 
#' in the refinement stage. Set it to 'compsket' for Algorithm 1, to 'proj' 
#' for Algorithm 2, and to 'lasso_bic' for Algorithm 3. The default is 
#' 'lasso_bic'. The variant employed in the NOT stage is always 'proj' in the 
#' current implementation.
#' @param burnIn the burnIn parameter to be passed to cpreg in both the NOT and 
#' refinements, which specifies the fraction at both ends of the interval to be 
#' discarded as possible changes, to handle common boundary effects.
#' @param verbose whether to output intermediate results/progress bar in the 
#' console
#' @return a list containing the initial changepoint estimates before the 
#' testing refinement, the intermediate estimates after the testing refinement, 
#' the intermediate estimates after the midpoint refinement and the estimates 
#' after the final refinement.
#' @description From the model y_t = x_t beta_t + eps_t for 1<=t<=z with 
#' beta_t = beta^i for z_(i-1) < t <= z_i, for 1 <= i <= nu + 1, where x_t 
#' are p-dimensional design vectors for 1<=t<=n, nu is the number of 
#' changepoints and z_i is the i-th changepoint with the convention z_0 = 0 
#' and z_(nu+1) = n.  We estimate the changepoints z_i's where the changes of 
#' regression coefficient take place by charcoal. The implementation contains 
#' Algorithm 4 and the post-processing discussed in the numerical section of 
#' Gao and Wang (2022).
#' @references Gao, F. and Wang, T. (2022) Sparse change detection in 
#' high-dimensional regression.
#' @references Baranowski, R., Chen, Y., & Fryzlewicz, P. (2019).
#'  Narrowest‐over‐threshold detection of multiple change points and 
#'  change‐point‐like features. Journal of the Royal Statistical Society: 
#'  Series B (Statistical Methodology), 81(3), 649-672.
#' @export
#' @examples
#' # problem parameters
#' n <- 240 # total sample size
#' tau <- c(0.3,0.55)  # changepoint location
#' p <- 80 # dimension of covariates
#' k <- c(3,10) # sparsity of the difference vector of the regression 
#' coefficients o
#' rho <- 1.5 # difference in l_2 norm of the two regression coefficients
#' # generate the design matrix and response vector of the linear regression 
#' sample with multiple changepoints
#' ret <- simulateMultiCP(n,tau,p,k,rho,sigma=1)
#' X <- ret$X; Y <- ret$Y
#' not_result <- not_cpreg(X,Y, Thresh=10)
#' # or  not_result <- cpreg(X,Y,Thresh=NULL,no_intervals=200,burnIn=0.05,
#' cpreg_method='proj')
not_cpreg <- function(X, Y, thresh=NULL,
                      no_intervals = floor(nrow(X)/5), burnIn=0, sigma=NULL,
                      verbose=FALSE, cpreg_method='lasso_bic', seed=NULL) {
  
  n <- nrow(X); p <- ncol(X)
  if (is.null(sigma)) {
    ret <- cpreg(X, Y)
    sigma <- mad(ret$Q)
  } else { X<-X/sigma; Y<-Y/sigma; sigma<-1}
  if (is.null(thresh)) {
    if (verbose) print("Generating threshold for NOT and testing")
    thresh_ret <- getTestThreshold(n, p, burnIn, alpha=0.01/no_intervals,
                                   permSize=1000, sigma = sigma,
                                   verbose=verbose)
    thresh <- thresh_ret$thresh
    if (verbose) print(paste0("Obtained threshold = ", thresh))
  }
  if (!is.null(seed)) set.seed(seed)
  
  # get intervals, left open, right closed intervals
  intervals <- getNOTIntervals(n, p, no_intervals, 0.1)
  
  not <- function(start, end){
    if (verbose) print(paste0('Searching interval: (', start, ', ', end, ']'))
    if (end - start <= p) return(NULL)
    
    M_se  <- intervals[intervals[,1] >= start & intervals[,2] <= end, ]
    M_se <- rbind(M_se, c(start, end))
    results <- data.frame(left=M_se[,1], right=M_se[,2])
    results$lengths <- results$right - results$left
    results$cps <- results$stats <- 0
    
    for (i in 1:nrow(results)){
      sm <- results$left[i]; em <- results$right[i]
      tmp <- cpreg(X[(sm+1):em,], Y[(sm+1):em], sigma=sigma, burnIn=burnIn)
      results$cps[i] <- tmp$cp + sm
      results$stats[i] <- tmp$stats[tmp$cp]
      results$sigma[i] <- mad(tmp$Q)
    }
    
    if (sum(results$stats >= thresh) == 0) return(NULL)
    results <- results[results$stats >= thresh, ]
    m_star <- which.min(results$lengths)
    cp <- results$cps[m_star]
    if (verbose)  print(c('discovered cp at', cp))
    
    ret <- setNames(c(cp, results$stats[m_star]),
                    c('cps','stats'))
    return(rbind(not(start, cp), ret, not(cp, end)))
  }
  
  # Stage 1: initial changepoint searching via NOT
  ret_initial <- not(0, n)
  if (is.null(ret_initial)) return(NULL)
  
  ret_initial <- as.data.frame(ret_initial, row.names=1:nrow(ret_initial))
  if (verbose) print(ret_initial)
  
  # Stage 2: perform tests on each identified changepoint
  ret_tested <- ret_initial
  for (i in order(ret_initial$stats)){
    cp <- ret_initial$cps[i]
    before.idx <- max(ret_tested$cps[ret_tested$cps < cp], 0)
    after.idx <- min(ret_tested$cps[ret_tested$cps > cp], n)
    if (verbose) print(paste0('testing cp at ', cp, ' in (',
                              before.idx, ', ', after.idx, ']'))
    
    too_small <- after.idx - before.idx < p + 10
    too_close <- min(cp - before.idx, after.idx - cp) < burnIn * n
    if (!too_small && !too_close){
      tmp <- cpreg(X[(before.idx+1):after.idx, ],
                   Y[(before.idx+1):after.idx], burnIn = burnIn)
      test_stat <- tmp$stats[tmp$cp] / sigma
      test_fail <- test_stat < thresh
    } else {
      test_fail <- FALSE
    }
    if (too_small || too_close || test_fail) {
      ret_tested <- ret_tested[-match(cp, ret_tested$cps), ]
    } else {
      ret_tested[match(cp, ret_tested$cps), 'stats'] <- test_stat
    }
    if (verbose) {
      if (too_small) {
        print('current interval too small, merged')
      } else if (too_close) {
        print('current cp close to boundary, merged')
      } else if (test_fail){
        print('test stat below threshold, removed')
      } else {
        print(paste0('keeping cp at ', cp))
      }
    }
  }
  if (verbose) print(ret_tested)
  
  # Stage 3: refine the change point estimate via midpoint refinement
  ret_refined <- ret_tested
  for (i in order(ret_tested$stats, decreasing=TRUE)) {
    cp <- ret_refined$cps[i]
    before.idx <- max(ret_refined$cps[ret_refined$cps < cp], 0)
    after.idx <- min(ret_refined$cps[ret_refined$cps > cp], n)
    before.mid <- floor((cp + before.idx) / 2)
    after.mid <- floor((cp + after.idx) / 2)
    
    if (verbose) print(paste0('midpoint refinement interval (',
                              before.mid, ', ', after.mid, ']'))
    
    if (after.mid - before.mid >= (1 + burnIn) * p) {
      tmp <- cpreg(X[(before.mid+1):after.mid, ],
                   Y[(before.mid+1):after.mid],
                   burnIn=0, sigma=sigma, aggregate_method=cpreg_method)
      ret_refined$cps[i] <- tmp$cp + before.mid
      ret_refined$stats[i] <- tmp$stats[tmp$cp]
      if (verbose) print(paste0('refining cp at ', cp, ' to new cp at ',
                                ret_refined$cps[i]))
    } else {
      ret_refined$stats[i] <- 0
      if (verbose) print('not big enough interval, skip first refinement')
    }
  }
  if (verbose) print(ret_refined)
  
  # Stage 4: second refinement
  ret_final <- ret_refined
  
  for (i in order(ret_tested$stats, decreasing=TRUE)) {
    cp <- ret_final$cps[i]
    burnInLen <- min(10, floor(burnIn*n))
    before.idx <- max(ret_final$cps[ret_final$cps < cp] + burnInLen, 0)
    after.idx <- min(ret_final$cps[ret_final$cps > cp] - burnInLen, n)
    if (after.idx - before.idx >= (1+burnIn) * p) {
      tmp <- cpreg(X[(before.idx+1):after.idx,],
                   Y[(before.idx+1):after.idx],
                   burnIn=burnIn, sigma=sigma, aggregate_method = cpreg_method)
      ret_final$cps[i] <- tmp$cp + before.idx
      ret_final$stats[i] <- tmp$stats[tmp$cp]
      if (verbose) print(paste('refining cp at', cp, 'to', ret_final$cps[i]))
    } else {
      ret_final$stats[i] <- 0
      if (verbose) print('not big enough interval, skip second refinement')
    }
  }
  if (verbose) print(ret_final)
  
  return(list(cps=ret_final$cps, initial=ret_initial, tested=ret_tested,
              refined=ret_refined, final=ret_final))
}
